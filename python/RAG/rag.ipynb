{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "088ac7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.0)\n",
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.13.1)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai faiss-cpu python-dotenv numpy pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bf9a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eddf42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Config ------------------\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY missing; set it in env or .env\")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "MARKDOWN_DIR = Path(\"markdown\")\n",
    "OUT = Path(\"out\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKS_JSONL = OUT / \"chunks.jsonl\"\n",
    "META_JSON = OUT / \"meta.json\"\n",
    "EMB_PATH = OUT / \"embeddings.npy\"\n",
    "IDS_PATH = OUT / \"ids.npy\"\n",
    "FAISS_INDEX_PATH = OUT / \"index.faiss\"\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "EMB_DIM_EXPECTED = 1536\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "IMAGE_MD_RE = re.compile(r'!\\[[^\\]]*\\]\\(([^)]+)\\)', flags=re.I)\n",
    "PAGE_MARKER_RE = re.compile(r'<!--\\s*PAGE\\s*\\d*\\s*-->', flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9184260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Helpers ------------------\n",
    "def short_id(base: str, page: int) -> str:\n",
    "    h = hashlib.sha1(f\"{base}:{page}\".encode(\"utf-8\")).hexdigest()\n",
    "    return f\"chunk_{h[:8]}\"\n",
    "\n",
    "def ensure_dir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d90474bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Data model ------------------\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str\n",
    "    source_file: str\n",
    "    converted_at: Optional[str]\n",
    "    notes: Optional[str]\n",
    "    filename: str\n",
    "    lang: Optional[str]\n",
    "    text: str\n",
    "    md: str\n",
    "    images: List[Dict[str,str]]\n",
    "    page: Optional[int]\n",
    "    start_char: int\n",
    "    end_char: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0dad609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Markdown parsing ------------------\n",
    "def extract_frontmatter(md_text: str) -> Tuple[Dict[str, Any], str]:\n",
    "    s = md_text\n",
    "    m = re.match(r'^\\s*---\\s*\\n(.*?)\\n---\\s*\\n(.*)$', s, flags=re.S)\n",
    "    if m:\n",
    "        fm_raw = m.group(1).strip()\n",
    "        remainder = m.group(2)\n",
    "    else:\n",
    "        parts = s.split('---', 2)\n",
    "        if len(parts) >= 3:\n",
    "            fm_raw = parts[1].strip()\n",
    "            remainder = parts[2]\n",
    "        else:\n",
    "            return {}, md_text\n",
    "    try:\n",
    "        fm = json.loads(fm_raw)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import yaml\n",
    "            fm = yaml.safe_load(fm_raw) or {}\n",
    "        except Exception:\n",
    "            fm = {}\n",
    "    return fm, remainder\n",
    "\n",
    "def split_pages(md_body: str) -> List[str]:\n",
    "    if PAGE_MARKER_RE.search(md_body):\n",
    "        parts = re.split(PAGE_MARKER_RE, md_body)\n",
    "        if parts and not parts[0].strip():\n",
    "            parts = parts[1:]\n",
    "        return [p.strip() for p in parts]\n",
    "    else:\n",
    "        return [md_body.strip()]\n",
    "\n",
    "def parse_page_images_and_text(page_md: str) -> Tuple[str, List[Dict[str,str]]]:\n",
    "    images: List[Dict[str,str]] = []\n",
    "    def repl(m):\n",
    "        link = m.group(1).strip()\n",
    "        images.append({\"link\": link, \"description\": \"\"})\n",
    "        return \" [image] \"\n",
    "    text = IMAGE_MD_RE.sub(repl, page_md)\n",
    "    text = text.replace(\"[image]\", \" [image] \")\n",
    "    text_for_emb = \" \".join(text.split())\n",
    "    return text_for_emb, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b352bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Chunk builders ------------------\n",
    "def build_chunks_from_markdown_file(md_path: Path, doc_prefix: Optional[str] = None) -> List[Chunk]:\n",
    "    if not md_path.exists():\n",
    "        print(f\"[build_chunks] missing file: {md_path}\")\n",
    "        return []\n",
    "    s = md_path.read_text(encoding=\"utf-8\")\n",
    "    fm, body = extract_frontmatter(s)\n",
    "    converted_at = fm.get(\"converted_at\")\n",
    "    notes = fm.get(\"notes\")\n",
    "    fm_source = fm.get(\"source_file\")\n",
    "    if fm_source:\n",
    "        src_name = str(fm_source)\n",
    "        if doc_prefix:\n",
    "            prefix = doc_prefix.rstrip('/')\n",
    "            if not src_name.startswith(f\"{prefix}/\"):\n",
    "                source_file = f\"{prefix}/{src_name}\"\n",
    "            else:\n",
    "                source_file = src_name\n",
    "        else:\n",
    "            source_file = src_name\n",
    "    else:\n",
    "        source_file = f\"{doc_prefix.rstrip('/')}/{md_path.name}\" if doc_prefix else md_path.name\n",
    "    lang = fm.get(\"lang\") if fm.get(\"lang\") else None\n",
    "    pages = split_pages(body)\n",
    "    chunks: List[Chunk] = []\n",
    "    for idx, page_md in enumerate(pages, start=1):\n",
    "        text_for_emb, images = parse_page_images_and_text(page_md)\n",
    "        cid = short_id(source_file, idx)\n",
    "        c = Chunk(\n",
    "            id=cid,\n",
    "            source_file=source_file,\n",
    "            converted_at=converted_at,\n",
    "            notes=notes,\n",
    "            filename=md_path.name,\n",
    "            lang=lang,\n",
    "            text=text_for_emb,\n",
    "            md=page_md.strip(),\n",
    "            images=images.copy(),\n",
    "            page=idx,\n",
    "            start_char=0,\n",
    "            end_char=len(text_for_emb)\n",
    "        )\n",
    "        chunks.append(c)\n",
    "    print(f\"[build_chunks] {md_path.name}: pages={len(pages)} chunks={len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def build_all_chunks(md_dir: Path = MARKDOWN_DIR, doc_prefix: Optional[str] = None) -> List[Chunk]:\n",
    "    md_files = sorted(md_dir.glob(\"*.md\"))\n",
    "    if not md_files:\n",
    "        print(f\"[warn] no markdown files in {md_dir.resolve()}\")\n",
    "        return []\n",
    "    all_chunks: List[Chunk] = []\n",
    "    for md in md_files:\n",
    "        all_chunks.extend(build_chunks_from_markdown_file(md, doc_prefix=doc_prefix))\n",
    "    # write chunks.jsonl\n",
    "    with CHUNKS_JSONL.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for c in all_chunks:\n",
    "            fh.write(json.dumps(asdict(c), ensure_ascii=False) + \"\\n\")\n",
    "    # build meta.json keyed by numeric id\n",
    "    meta = {}\n",
    "    for c in all_chunks:\n",
    "        nid = int(hashlib.sha1(c.id.encode()).hexdigest()[:15], 16)\n",
    "        meta[str(nid)] = asdict(c)\n",
    "    META_JSON.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"[out] wrote {len(all_chunks)} chunks -> {CHUNKS_JSONL}, meta -> {META_JSON}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9351441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Embeddings ------------------\n",
    "def embed_batch_safe(texts: List[str], model: str = EMBED_MODEL, max_retries:int = 6, backoff_base: float = 0.5):\n",
    "    tries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=model, input=texts)\n",
    "            out = []\n",
    "            for item in resp.data:\n",
    "                emb = getattr(item, \"embedding\", None) or (item.get(\"embedding\") if isinstance(item, dict) else None)\n",
    "                if emb is None:\n",
    "                    raise RuntimeError(\"embedding parse error\")\n",
    "                out.append(emb)\n",
    "            return out\n",
    "        except Exception as e:\n",
    "            tries += 1\n",
    "            if tries > max_retries:\n",
    "                raise\n",
    "            wait = backoff_base * (2 ** (tries-1))\n",
    "            print(f\"[emb] retry {tries}/{max_retries}, sleeping {wait:.1f}s: {e}\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "def build_or_resume_embeddings(chunks: List[Chunk], batch_size:int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if not chunks:\n",
    "        raise RuntimeError(\"No chunks to embed.\")\n",
    "    numeric_ids = [int(hashlib.sha1(c.id.encode()).hexdigest()[:15], 16) for c in chunks]\n",
    "    existing_map: Dict[int, np.ndarray] = {}\n",
    "    if EMB_PATH.exists() and IDS_PATH.exists():\n",
    "        try:\n",
    "            arr = np.load(EMB_PATH)\n",
    "            ids = np.load(IDS_PATH).astype(np.int64)\n",
    "            if arr.ndim == 2 and arr.shape[1] == EMB_DIM_EXPECTED and ids.shape[0] == arr.shape[0]:\n",
    "                for i, nid in enumerate(ids):\n",
    "                    existing_map[int(nid)] = arr[i].astype(np.float32)\n",
    "                print(f\"[emb] loaded existing embeddings into map: {len(existing_map)} entries\")\n",
    "            else:\n",
    "                print(\"[emb] existing embeddings file shape mismatch — ignoring\")\n",
    "        except Exception as e:\n",
    "            print(\"[emb] failed to load existing embeddings — ignoring:\", e)\n",
    "            existing_map = {}\n",
    "\n",
    "    final_vecs: List[Optional[np.ndarray]] = [None] * len(numeric_ids)\n",
    "    to_compute_indices: List[int] = []\n",
    "    for i, nid in enumerate(numeric_ids):\n",
    "        v = existing_map.get(nid)\n",
    "        if v is not None:\n",
    "            final_vecs[i] = v\n",
    "        else:\n",
    "            to_compute_indices.append(i)\n",
    "\n",
    "    print(f\"[emb] total_chunks={len(chunks)} reuse={len(chunks)-len(to_compute_indices)} compute={len(to_compute_indices)}\")\n",
    "\n",
    "    for i in range(0, len(to_compute_indices), batch_size):\n",
    "        batch_idx = to_compute_indices[i:i+batch_size]\n",
    "        batch_texts = [chunks[j].text if chunks[j].text.strip() else \" \" for j in batch_idx]\n",
    "        print(f\"[emb] computing batch {i//batch_size+1}: size {len(batch_texts)}\")\n",
    "        out = embed_batch_safe(batch_texts)\n",
    "        for k, pos in enumerate(batch_idx):\n",
    "            final_vecs[pos] = np.array(out[k], dtype=np.float32)\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    missing = [i for i, v in enumerate(final_vecs) if v is None]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[emb] missing embeddings for indices: {missing}\")\n",
    "\n",
    "    arr = np.vstack([v.reshape(1, -1) for v in final_vecs]).astype(np.float32)\n",
    "    ids_arr = np.array(numeric_ids, dtype=np.int64)\n",
    "\n",
    "    # overwrite saved embeddings/ids so they match current chunks order\n",
    "    np.save(EMB_PATH, arr)\n",
    "    np.save(IDS_PATH, ids_arr)\n",
    "    print(f\"[emb] saved embeddings -> {EMB_PATH} shape={arr.shape}, ids -> {IDS_PATH} count={ids_arr.shape[0]}\")\n",
    "\n",
    "    return arr, ids_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8fa1c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ FAISS ------------------\n",
    "def build_and_save_faiss(emb_arr: np.ndarray, emb_ids: np.ndarray, index_path: Path = FAISS_INDEX_PATH):\n",
    "    if emb_arr is None or emb_arr.shape[0] == 0:\n",
    "        raise RuntimeError(\"No embeddings to index\")\n",
    "    arr_copy = emb_arr.astype(np.float32, copy=True)\n",
    "    faiss.normalize_L2(arr_copy)\n",
    "    dim = arr_copy.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    idmap = faiss.IndexIDMap(index)\n",
    "    idmap.add_with_ids(arr_copy, emb_ids.astype(np.int64))\n",
    "    faiss.write_index(idmap, str(index_path))\n",
    "    print(f\"[faiss] saved index -> {index_path} (ntotal={idmap.ntotal})\")\n",
    "    return idmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80e64a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Search ------------------\n",
    "def load_meta(out_dir: Path = OUT) -> Dict[str,Any]:\n",
    "    if not (out_dir / \"meta.json\").exists():\n",
    "        return {}\n",
    "    return json.loads((out_dir / \"meta.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def embed_query(q: str, model: str = EMBED_MODEL) -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=model, input=[q])\n",
    "    item = resp.data[0]\n",
    "    emb = getattr(item, \"embedding\", None) or (item.get(\"embedding\") if isinstance(item, dict) else None)\n",
    "    if emb is None:\n",
    "        raise RuntimeError(\"Can't parse embedding response\")\n",
    "    arr = np.array(emb, dtype=np.float32)\n",
    "    return arr\n",
    "\n",
    "def search_topk(query: str, top_k: int = 5, index: faiss.Index = None, meta: Dict[str,Any] = None):\n",
    "    if index is None:\n",
    "        if not FAISS_INDEX_PATH.exists():\n",
    "            raise RuntimeError(\"FAISS index not found\")\n",
    "        idx = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "    else:\n",
    "        idx = index\n",
    "    if meta is None:\n",
    "        meta = load_meta(OUT)\n",
    "    if idx.ntotal == 0:\n",
    "        print(\"[search] empty index\")\n",
    "        return []\n",
    "    q = embed_query(query)\n",
    "    q_arr = np.array([q], dtype=np.float32)\n",
    "    faiss.normalize_L2(q_arr)\n",
    "    search_k = min(max(64, top_k * 8), max(1, int(idx.ntotal)))\n",
    "    print(f\"[search] idx.ntotal={idx.ntotal} top_k={top_k} search_k={search_k}\")\n",
    "    D, I = idx.search(q_arr, search_k)\n",
    "    results = []\n",
    "    for sc, nid in zip(D[0].tolist(), I[0].tolist()):\n",
    "        if nid == -1:\n",
    "            continue\n",
    "        key = str(int(nid))\n",
    "        m = meta.get(key)\n",
    "        if not m:\n",
    "            # still warn (should not happen if emb/ids/meta aligned)\n",
    "            print(f\"[search] warn: missing meta for id {key}\")\n",
    "            continue\n",
    "        results.append({\"score\": float(sc), \"meta\": m})\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba5364ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline start. MARKDOWN_DIR: /Users/rakymzhan/Documents/projects/src/distributed-agent/python/RAG/markdown OUT: /Users/rakymzhan/Documents/projects/src/distributed-agent/python/RAG/out\n",
      "[build_chunks] application_ru.md: pages=10 chunks=10\n",
      "[build_chunks] residence_permit_ru.md: pages=10 chunks=10\n",
      "[out] wrote 20 chunks -> out/chunks.jsonl, meta -> out/meta.json\n",
      "[emb] loaded existing embeddings into map: 20 entries\n",
      "[emb] total_chunks=20 reuse=20 compute=0\n",
      "[emb] saved embeddings -> out/embeddings.npy shape=(20, 1536), ids -> out/ids.npy count=20\n",
      "[main] embeddings shape: (20, 1536) ids shape: (20,)\n",
      "[faiss] saved index -> out/index.faiss (ntotal=20)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Как подать на ВНЖ?\n",
      "[search] idx.ntotal=20 top_k=5 search_k=20\n",
      "01) score=0.5205  file=italy/residence_permit_ru.pdf  page=10\n",
      "    snippet: Этап 3: Забираете свой ВНЖ с полицейского участка [image] Permesso di Soggiorno пример Приносите свой паспорт и все бумажки с почты, которые вам дали изначально. Адрес будет таким же как и в этапе 2. Вам придет сообщение...\n",
      "02) score=0.5026  file=italy/residence_permit_ru.pdf  page=1\n",
      "    snippet: Подача на ВНЖ (Permesso di Soggiorno) в Италии Вид на жительство – это документ государственного образца, дающий право легального нахождения в ее территориальных пределах. Выдача временного вида на жительство в Италии ос...\n",
      "03) score=0.4710  file=italy/residence_permit_ru.pdf  page=8\n",
      "    snippet: После заполнения берете талончик на почте, и по продвижению вашей очереди - отдаете конверт с документами работнику почты. При вас этот конверт распаковывают, проверяют правильность заполнения и документы, клеят марку и ...\n",
      "04) score=0.4607  file=italy/residence_permit_ru.pdf  page=3\n",
      "    snippet: Этап 1. Часть 2: Заполнение ВНЖ После того, как вы взяли свой конверт, начните его заполнять черной ручкой. Вам нужно заполнить только MODULO 1 и бумажку. Заполняете все черной ручкой и большими ЗАГЛАВНЫМИ буквами. Все п...\n",
      "05) score=0.4333  file=italy/application_ru.pdf  page=7\n",
      "    snippet: ПРОЦЕСС ПОДАЧИ ЗАЯВКИ Чтобы узнать, как подать заявку в конкретный университет, важно посетить официальный сайт учебного заведения. В разделе \"Admission\" или \"International students\" будет указана вся информация о процес...\n",
      "\n",
      "Query: Где купить Marca da Bollo?\n",
      "[search] idx.ntotal=20 top_k=5 search_k=20\n",
      "01) score=0.3862  file=italy/residence_permit_ru.pdf  page=2\n",
      "    snippet: Этап 1. Часть 1 (в первые 8 дней по приезду): Первым делом вам нужно пойти в Табаccheria (Табакерия) и купить одну штуку Marca da Bollo (марку) пo 16 евро, больше не нужно. [image] Marca da Bollo stamp on shop counter [i...\n",
      "02) score=0.2146  file=italy/application_ru.pdf  page=10\n",
      "    snippet: ВОТ НЕСКОЛЬКО ССЫЛОК ДЛЯ ПОИСКА И ПОДАЧИ ЗАЯВОК В УНИВЕРСИТЕТЫ ИТАЛИИ: • Universitaly (официальный портал для подачи заявок в университеты Италии): www.universitaly.it • Study in Italy (государственный сайт, предоставляю...\n",
      "03) score=0.2092  file=italy/application_ru.pdf  page=1\n",
      "    snippet: ОСОБЕННОСТИ ПОСТУПЛЕНИЯ Для поступление в Италию (бакалавриат) необходимо наличие 12 летнего образования. • После 11-го класса необходимо закончить 1 год обучения в университете и далее можно будет поступать в Италию или...\n",
      "04) score=0.2068  file=italy/residence_permit_ru.pdf  page=7\n",
      "    snippet: Этап 1. Часть 3: Готовим конверт к подаче Вам нужно оставить MODULO 1, MODULO 2 и маленькую бумажку. Все остальные ч/б бумажки можете выбросить. Также, необходимо будет положить в конверт: 1. Одна штука Marca da Bollo; 2...\n",
      "05) score=0.1872  file=italy/residence_permit_ru.pdf  page=9\n",
      "    snippet: Этап 2: Сдача отпечатков пальцев Что нужно принести для сдачи отпечатков пальцев? 1. Паспорт; 2. Копии контракт на жилье (на 10 месяцев минимум) + копия регистрации контракта в Agenzia delle entrate (в Риме и некоторых к...\n",
      "\n",
      "Query: Как подать в университет?\n",
      "[search] idx.ntotal=20 top_k=5 search_k=20\n",
      "01) score=0.6760  file=italy/application_ru.pdf  page=7\n",
      "    snippet: ПРОЦЕСС ПОДАЧИ ЗАЯВКИ Чтобы узнать, как подать заявку в конкретный университет, важно посетить официальный сайт учебного заведения. В разделе \"Admission\" или \"International students\" будет указана вся информация о процес...\n",
      "02) score=0.6262  file=italy/application_ru.pdf  page=9\n",
      "    snippet: ЧАСТЫЕ ОШИБКИ И КАК ИХ ИЗБЕЖАТЬ 1. Несоблюдение сроков подачи заявки Решение: Создайте календарь с дедлайнами для каждого университета и программы и следите за ними заранее. 2. Недостаточная документация Решение: Внимате...\n",
      "03) score=0.6020  file=italy/application_ru.pdf  page=8\n",
      "    snippet: PRE-ENROLLMENT (ПРЕДВАРИТЕЛЬНАЯ РЕГИСТРАЦИЯ) • Регистрация на платформе Universitaly: • Заходите на сайт Universitaly (www.universitaly.it). • Создаёте личный кабинет, выбираете программу обучения, загружаете необходимые...\n",
      "04) score=0.5711  file=italy/application_ru.pdf  page=3\n",
      "    snippet: ПОДГОТОВКА ДОКУМЕНТОВ В зависимости от университета и программы список документов может отличаться, но основные из них: • Диплом бакалавра или справка о текущем обучении (если вы на последнем курсе, переведенный на англи...\n",
      "05) score=0.5127  file=italy/application_ru.pdf  page=6\n",
      "    snippet: Внутренние экзамены университета Некоторые университеты также могут требовать сдачи внутренних экзаменов, которые проводятся непосредственно самим университетом. Эти экзамены могут включать в себя: • Тесты по общей подго...\n",
      "\n",
      "Query: How do I apply to university?\n",
      "[search] idx.ntotal=20 top_k=5 search_k=20\n",
      "01) score=0.5529  file=italy/application_ru.pdf  page=7\n",
      "    snippet: ПРОЦЕСС ПОДАЧИ ЗАЯВКИ Чтобы узнать, как подать заявку в конкретный университет, важно посетить официальный сайт учебного заведения. В разделе \"Admission\" или \"International students\" будет указана вся информация о процес...\n",
      "02) score=0.5229  file=italy/application_ru.pdf  page=8\n",
      "    snippet: PRE-ENROLLMENT (ПРЕДВАРИТЕЛЬНАЯ РЕГИСТРАЦИЯ) • Регистрация на платформе Universitaly: • Заходите на сайт Universitaly (www.universitaly.it). • Создаёте личный кабинет, выбираете программу обучения, загружаете необходимые...\n",
      "03) score=0.4753  file=italy/application_ru.pdf  page=9\n",
      "    snippet: ЧАСТЫЕ ОШИБКИ И КАК ИХ ИЗБЕЖАТЬ 1. Несоблюдение сроков подачи заявки Решение: Создайте календарь с дедлайнами для каждого университета и программы и следите за ними заранее. 2. Недостаточная документация Решение: Внимате...\n",
      "04) score=0.4749  file=italy/application_ru.pdf  page=2\n",
      "    snippet: ВЫБОР УНИВЕРСИТЕТА И ПРОГРАММЫ Прежде чем начинать процесс поступления, важно выбрать подходящий университет и программу. В Италии есть как государственные, так и частные вузы, предлагающие обучение на английском и италь...\n",
      "05) score=0.4712  file=italy/application_ru.pdf  page=3\n",
      "    snippet: ПОДГОТОВКА ДОКУМЕНТОВ В зависимости от университета и программы список документов может отличаться, но основные из них: • Диплом бакалавра или справка о текущем обучении (если вы на последнем курсе, переведенный на англи...\n",
      "\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Main ------------------\n",
    "def main():\n",
    "    print(\"RAG pipeline start. MARKDOWN_DIR:\", MARKDOWN_DIR.resolve(), \"OUT:\", OUT.resolve())\n",
    "    chunks = build_all_chunks(MARKDOWN_DIR)\n",
    "    if not chunks:\n",
    "        raise RuntimeError(\"No chunks produced. Check markdown files and MARKDOWN_DIR.\")\n",
    "    emb_arr, emb_ids = build_or_resume_embeddings(chunks, batch_size=min(BATCH_SIZE, 32))\n",
    "    print(\"[main] embeddings shape:\", getattr(emb_arr, \"shape\", None), \"ids shape:\", getattr(emb_ids, \"shape\", None))\n",
    "    index = build_and_save_faiss(emb_arr, emb_ids)\n",
    "    meta = load_meta(OUT)\n",
    "\n",
    "    # quick smoke queries (modify these or remove)\n",
    "    queries = [\n",
    "        \"Как подать на ВНЖ?\",\n",
    "        \"Где купить Marca da Bollo?\",\n",
    "        \"Как подать в университет?\",\n",
    "        \"How do I apply to university?\"\n",
    "    ]\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    for q in queries:\n",
    "        print(\"\\nQuery:\", q)\n",
    "        res = search_topk(q, top_k=5, index=index, meta=meta)\n",
    "        if not res:\n",
    "            print(\" -> No results\")\n",
    "            continue\n",
    "        for i, r in enumerate(res, start=1):\n",
    "            m = r[\"meta\"]\n",
    "            score = r[\"score\"]\n",
    "            # PREFER the frontmatter source_file (it may include path like 'italy/application_ru.pdf')\n",
    "            fname = m.get(\"source_file\") or m.get(\"filename\")\n",
    "            page = m.get(\"page\", \"n/a\")\n",
    "            snippet = (m.get(\"text\") or m.get(\"md\") or \"\")[:400].replace(\"\\n\", \" \").strip()\n",
    "            print(f\"{i:02d}) score={score:.4f}  file={fname}  page={page}\")\n",
    "            print(\"    snippet:\", (snippet[:220] + (\"...\" if len(snippet) > 220 else \"\")))\n",
    "    print(\"\\n[done]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a71ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
